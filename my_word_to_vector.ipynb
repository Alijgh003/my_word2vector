{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhPUxh58WZXCJ967mIrmwD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alijgh003/my_word2vector/blob/main/my_word_to_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "-00mg_d8qVXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "KbJl0hYTqW4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_data = load_dataset(\"imdb\")\n",
        "imdb_data"
      ],
      "metadata": {
        "id": "68NUGgt2qbmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = imdb_data['unsupervised'][:5000]"
      ],
      "metadata": {
        "id": "fEBJK5ODqu3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "s0tJRY97rgPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "def tokenize_and_normalize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return  tokens\n",
        "\n",
        "sample_text = \"Hello! This is an example sentence. Let's see how it gets tokenized.\"\n",
        "tokens = tokenize_and_normalize(sample_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "53gV5MQXriFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "corpus = [tokenize_and_normalize(row) for row in train_dataset['text']]"
      ],
      "metadata": {
        "id": "9D0xLdLdry1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_to_distinct_words(corpus):\n",
        "  words = sorted(list({word for sublist in corpus for word in sublist}))\n",
        "  return words, len(words)\n",
        "\n",
        "distinct_words, num_of_distinct_words = corpus_to_distinct_words(corpus)\n",
        "distinct_words = ['<START>'] + ['<END>'] + distinct_words\n",
        "num_of_distinct_words += 2\n",
        "word_to_index = {word: index for index, word in enumerate(distinct_words)}\n",
        "\n",
        "print(f'distinct words: {distinct_words[10000:10011]}')\n",
        "print(f'num of distinct words: {num_of_distinct_words}')"
      ],
      "metadata": {
        "id": "t0W_XjA4uuVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "kqathqXtwCbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 3"
      ],
      "metadata": {
        "id": "stLLhrCZx6Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "centers, outsides = [], []\n",
        "for text in corpus:\n",
        "  before_center = [word_to_index['<START>']] * WINDOW_SIZE\n",
        "  for i, word in enumerate(text):\n",
        "    down = max(0, i-WINDOW_SIZE)\n",
        "    up = min(len(text), i+WINDOW_SIZE+1)\n",
        "    centers.append(word_to_index[word])\n",
        "    before_center = [word_to_index[w] for w in (WINDOW_SIZE-(i-down)) * ['<START>'] + text[down: i]]\n",
        "    after_center = [word_to_index[w] for w in text[i+1: up] + ['<END>'] * (WINDOW_SIZE-(up-1-i))]\n",
        "    outsides.append(before_center + after_center)\n",
        "\n",
        "print(len(outsides))\n",
        "centers = torch.tensor(centers).view(-1,1)\n",
        "outsides = torch.tensor(outsides)"
      ],
      "metadata": {
        "id": "MIe4olT8w3Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centers.shape, outsides.shape"
      ],
      "metadata": {
        "id": "TyK-DLFg8uJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_samples(total_samples, positive_indices , num_of_negative_samples):\n",
        "  probabilities = torch.ones((positive_indices.shape[0], total_samples))\n",
        "  probabilities[torch.arange(positive_indices.shape[0]).view(-1,1).repeat(1,positive_indices.shape[1]),positive_indices] = 0\n",
        "  probabilities = probabilities / probabilities.sum(dim=-1,keepdim=True)\n",
        "  sampled_index = torch.multinomial(probabilities, num_of_negative_samples)\n",
        "\n",
        "  return sampled_index"
      ],
      "metadata": {
        "id": "KDGLKKPWFHa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "get_negative_samples(num_of_distinct_words, outsides[:100], 600).shape"
      ],
      "metadata": {
        "id": "0rteQ_Xt0Ucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centers.shape, outsides.shape"
      ],
      "metadata": {
        "id": "aSoIvCok4y0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBD_SIZE =100"
      ],
      "metadata": {
        "id": "dx3nKlezkFSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "\n",
        "class My_Word2Vec(nn.Module):\n",
        "    def __init__(self, num_of_distinct_words, emb_dim ):\n",
        "        super(My_Word2Vec, self).__init__()\n",
        "        self.U = nn.Embedding(num_embeddings=num_of_distinct_words, embedding_dim=emb_dim)\n",
        "        self.V = nn.Embedding(num_embeddings=num_of_distinct_words, embedding_dim=emb_dim)\n",
        "\n",
        "    def forward(self, centers, outsides):\n",
        "        center_embedding = self.U.weight[centers]\n",
        "        context_embedding = self.V.weight[outsides]\n",
        "        return (center_embedding * context_embedding).sum(dim=-1)\n",
        "\n",
        "    def loss_f(self, predictions, labels):\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        loss = criterion(predictions, labels)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "vJEOKlcCk-AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "BKzLlo7u65kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(centers, outsides, max_epoch, num_of_distinct_words,model,optimizer,batch_size = 100):\n",
        "  model.train()\n",
        "  dataset = TensorDataset(centers, outsides)\n",
        "\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "  for epoch in range(max_epoch):\n",
        "    i = 0\n",
        "\n",
        "    for batch_features, batch_labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{max_epoch}\", ncols=100):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        neg_samples = get_negative_samples(num_of_distinct_words, batch_labels, batch_labels.shape[-1]*100).to(device)\n",
        "        context = torch.cat((batch_labels,neg_samples), dim=-1).to(device)\n",
        "\n",
        "        predictions = model(batch_features, context)\n",
        "\n",
        "        positive_labels = torch.ones_like(batch_labels).to(device)\n",
        "        negative_lables = torch.zeros_like(neg_samples).to(device)\n",
        "        labels = torch.cat((positive_labels, negative_lables), dim=-1).to(torch.float32).to(device)\n",
        "\n",
        "        loss = model.loss_f(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if(i%10 == 0):\n",
        "          print(f'{i=}, loss={loss.item():.4f}')\n",
        "        i += 1\n",
        "    print(f\"Epoch [{epoch+1}/{max_epoch}], Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RjEM5BmvzLst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_Word2Vec(num_of_distinct_words, EMBD_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
        "model.to(device)\n",
        "\n",
        "train(centers, outsides, 100, num_of_distinct_words, model, optimizer,1000)"
      ],
      "metadata": {
        "id": "XJ1nAbQP8sGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}